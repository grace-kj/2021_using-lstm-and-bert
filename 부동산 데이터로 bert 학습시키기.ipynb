{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @samsik_c: 작년에, 연말까지는 무조건 집값이 떨어진다고 정부가 나서서...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @wonnney: 대부분의 사람들이 모르는게.. 서울에 집을 많이 지으면 서울...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>나도 다달이 집값으로 나가는 대출 이자 생각하면???\\n(심지어 자가도 아닌\\n절대...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'미친 집값' 안떨어지는 이유, 중학교 사회교과서 답있다 [뉴스원샷] | 다음뉴스 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>\"시체 위에 지어라\" 반발도···'미친 집값' 떨어지지 않는 이유 https://t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document  label\n",
       "0    1.420000e+18  RT @samsik_c: 작년에, 연말까지는 무조건 집값이 떨어진다고 정부가 나서서...    1.0\n",
       "1    1.420000e+18  RT @wonnney: 대부분의 사람들이 모르는게.. 서울에 집을 많이 지으면 서울...    1.0\n",
       "2    1.420000e+18  나도 다달이 집값으로 나가는 대출 이자 생각하면???\\n(심지어 자가도 아닌\\n절대...    0.0\n",
       "3    1.420000e+18  '미친 집값' 안떨어지는 이유, 중학교 사회교과서 답있다 [뉴스원샷] | 다음뉴스 ...    1.0\n",
       "4    1.420000e+18  \"시체 위에 지어라\" 반발도···'미친 집값' 떨어지지 않는 이유 https://t...    1.0\n",
       "..            ...                                                ...    ...\n",
       "194  1.420000e+18  RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...    1.0\n",
       "195  1.420000e+18  RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...    1.0\n",
       "196  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...    1.0\n",
       "197  1.420000e+18  英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...    1.0\n",
       "198  1.420000e+18  '중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...    1.0\n",
       "\n",
       "[199 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= pd.read_csv('C:\\\\Users\\\\AAA\\\\Desktop\\\\dev\\\\트위터 api\\\\house price.csv', encoding='cp949')\n",
    "dataset= dataset.drop(['Datetime'], axis=1)\n",
    "dataset.columns=['id','document','label']\n",
    "dataset= dataset[:199]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.astype({'label':'int64'})\n",
    "dataset_train= dataset[:150]\n",
    "dataset_test=dataset[150:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document  label\n",
       "194  1.420000e+18  RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...      1\n",
       "195  1.420000e+18  RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...      1\n",
       "196  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1\n",
       "197  1.420000e+18  英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...      1\n",
       "198  1.420000e+18  '중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] RT @samsik_c: 작년에, 연말까지는 무조건 집값이 떨어진다고 정부가 나서서 국민들에게 집사지 마라던 대한민국 얘기 맞지요?\\n\\nhttps://t.co/lHflbVSRQW [SEP]',\n",
       " '[CLS] RT @wonnney: 대부분의 사람들이 모르는게.. 서울에 집을 많이 지으면 서울 집값은 더 많이 오르게 되어있다는 거...\\n\\n수요공급 법칙으로 되는게 아니라는걸 왜 모를까...\\n\\n집을 더 많이 지으면 그 지역 인구가 더 늘고, 그게 수요를 더 밀… [SEP]',\n",
       " '[CLS] 나도 다달이 집값으로 나가는 대출 이자 생각하면???\\n(심지어 자가도 아닌\\n절대 집에 있어\\n뽕 뽑아야해 [SEP]',\n",
       " \"[CLS] '미친 집값' 안떨어지는 이유, 중학교 사회교과서 답있다 [뉴스원샷] | 다음뉴스 https://t.co/a9tHxkZISm \\n중학교 수준?? [SEP]\",\n",
       " '[CLS] \"시체 위에 지어라\" 반발도···\\'미친 집값\\' 떨어지지 않는 이유 https://t.co/SdLOHJRFOI [SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in dataset_train.document]\n",
    "document_bert[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'RT', '@', 'sam', '##sik', '_', 'c', ':', '작', '##년에', ',', '연', '##말', '##까지', '##는', '무', '##조', '##건', '집', '##값', '##이', '떨', '##어진', '##다고', '정', '##부가', '나', '##서', '##서', '국', '##민', '##들에게', '집', '##사', '##지', '마', '##라', '##던', '대한민국', '얘', '##기', '맞', '##지', '##요', '?', 'https', ':', '/', '/', 't', '.', 'co', '/', 'l', '##H', '##f', '##lb', '##V', '##SR', '##Q', '##W', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#토크나이징\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n",
    "print(tokenized_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,  56898,    137,  21083,  41224,    168,    171,    131,\n",
       "         9652,  27056,    117,   9568,  89523,  18382,  11018,   9294,\n",
       "        20626,  71439,   9711, 118611,  10739,   9141,  46572,  85634,\n",
       "         9670,  81896,   8982,  12424,  12424,   8909,  36553,  61688,\n",
       "         9711,  12945,  12508,   9246,  17342,  23990,  26168,   9545,\n",
       "        12310,   9256,  12508,  48549,    136,  14120,    131,    120,\n",
       "          120,    188,    119,  11170,    120,    180,  12396,  10575,\n",
       "        70832,  11779,  52238,  19282,  13034,    102,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#패딩\n",
    "#token들의 max length보다 크게 MAX_LEN을 설정합니다.\n",
    "#설정한 MAX_LEN 만큼 빈 공간을 0이 채웁니다\n",
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#어텐션 마스크\n",
    "#학습속도를 높이기 위해 실 데이터가 있는 곳과 padding이 있는 곳을 attention에게 알려줍니다.\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "    \n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = \\\n",
    "train_test_split(input_ids, dataset_train['label'].values, random_state=42, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=42, \n",
    "                                                       test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이토치 텐서로 변환\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치 및 데이터로더 설정\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 셋에도 똑같이 적용\n",
    "\n",
    "sentences = dataset_test['document']\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "labels = dataset_test['label'].values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "#모델 학습\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('C:\\\\Users\\\\AAA\\\\Desktop\\\\dev\\\\트위터 api\\\\model.pt')\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습스케쥴링\n",
    "#transformers에서 제공하는 옵티마이저 중 AdamW를 사용합니다.\n",
    "#총 훈련 스텝은 이터레이션 * 에폭수로 설정해둡니다.\n",
    "#러닝 레잇 스케쥴러는 역시 transformers에서 제공하는 것을 사용합니다.\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# lr 조금씩 감소시키는 스케줄러\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습\n",
    "#accuracy와 시간함수\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.42\n",
      "  Training epcoh took: 0:00:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#학습 실행부분\n",
    "#데이터로더에서 배치만큼 가져온 후 forward, backward pass를 수행합니다.\n",
    "#gradient update는 명시적으로 하지 않고 위에서 로드한 optimizer를 활용합니다.\n",
    "\n",
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'modeltrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>“안타깝지만 집값 안 떨어집니다, 더 오를 겁니다” : 뉴스 : 동아닷컴 https...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>버블세븐 때처럼 '집값 거품론' 펴는 정부, 그런데도 시장은 정반대로 가고 있다 [...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>https://t.co/X2BPCzsDxy\\n공급폭탄이 정답입니다.\\n오르는 집값 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>정부에서 아파트 가격 상한선을 결정하는 중국, 폭등 집값을 40%↓ https://...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document  label\n",
       "150  1.420000e+18  “안타깝지만 집값 안 떨어집니다, 더 오를 겁니다” : 뉴스 : 동아닷컴 https...      1\n",
       "151  1.420000e+18  버블세븐 때처럼 '집값 거품론' 펴는 정부, 그런데도 시장은 정반대로 가고 있다 [...      1\n",
       "152  1.420000e+18  https://t.co/X2BPCzsDxy\\n공급폭탄이 정답입니다.\\n오르는 집값 ...      0\n",
       "153  1.420000e+18  RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...      0\n",
       "154  1.420000e+18  정부에서 아파트 가격 상한선을 결정하는 중국, 폭등 집값을 40%↓ https://...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.65\n",
      "Test took: 0:00:01\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "lab=[]\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    lab.append(label_ids[0])\n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaa\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>“안타깝지만 집값 안 떨어집니다, 더 오를 겁니다” : 뉴스 : 동아닷컴 https...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>버블세븐 때처럼 '집값 거품론' 펴는 정부, 그런데도 시장은 정반대로 가고 있다 [...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>https://t.co/X2BPCzsDxy\\n공급폭탄이 정답입니다.\\n오르는 집값 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>정부에서 아파트 가격 상한선을 결정하는 중국, 폭등 집값을 40%↓ https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @S710_VID: 마당 있는 큰 집 나올 때마다 부러워하며 가격부터 물어보는...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @hbeekay: @NYjjNY 맞는 말씀이시네요. ‘위기가 기회’라는 말을 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>2Q 집값 서울은 부진전세 상승세 지속</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>＂해외 유학 대신＂ 강남 부자들 우르르…13억 뚫은 제주 집값 (출처 : 한국경제 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @nonunsea: 재산서 고지서 날아왔는데 저년 대비 20% 경감된거 같다....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>@oo_moo1 집값 나눠내자^^</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>대전하수구 여수오픈행사 사상욕실인테리어 부안군혈당계 무주비만클리닉 홍천유리복원 대명...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @imnabyim: 만양 집값 오르겠네 이거</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>만양 집값 오르겠네 이거</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @nonunsea: 재산서 고지서 날아왔는데 저년 대비 20% 경감된거 같다....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>이정부처럼 집값잡겠다고???\\n\\n땅이 나라것인 공산당 나라에 집값이 평당 2억이란...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>&amp;lt;옆집에 수상한 과부가 살고있습니다?!&amp;gt;\\n\\n자수성가하여 한적한 라이프...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'집값 전쟁' 놓고 국회서 설전…참여정부 인사는 쓴소리 / 연합뉴스 (Yonhapn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>아니 근데 내가 이걸 웃을수가 없는게.. 실제로 베이징 집값이 너무 어마어마해서 주...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>20세기나 노무현때는 본질적으로 수도권의 압도적인 팽창이 집값 상승의 가장 큰 부분...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>예나 지금이나 대충 그래도 돈 좀 모았음의 기준이 1억인데 그거가지고 집샀으면 몇억...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>@NYjjNY 맞는 말씀이시네요. ‘위기가 기회’라는 말을 이따위로 쓰다니… 집투로...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>음 일단 시즈는 이혼녀임. 왜냐 그게더 맛있으니까\\n\\n이혼후 위자료랑 이것저것 대...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>집값이 더 오르는 말든 지금 상승기 한번 거하게 지나고 단기고점인데 지금 들어가도 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>조기숙 “文, 집값 폭락 사지 말라 언급”…내용은? | 뉴스A 라이브 - YouTu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>조기숙 \"문 대통령, 日처럼 집값 폭락할테니 집 사지 말라 해\" / YTN - Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>선택지는 두 가지임\\n\\n1. 테이퍼링, 금리인상 등으로 집값을 내려오게 유도하던가...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>정말 딱 집값만 다 갚으면 나는 공방차려야지 공방하고 평생 살래</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>\"중국판 대치동'에 폭등..\\n\\n최근 베이징 등 주요 도시에서 좋은 학교에 진학할...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>이렇게 매미도 많고 모기도 많고 에어컨 방에 한개씩 없는 집인데 집값은 더럽게 높네</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>윤은 그냥 평범한 회사원이엇는데 최근에 회사가 테러 집단 때문에 망해서 지금은 일단...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>요즘 지옥 집값이 너무 살신적이라~\\n1원 더 늘리는 부동산 정책을 필까 하는데 말야~</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>심즈 프리플레이 가구수 늘어날때마다 집값오르는거 개빡침..현실의 부동산이 게임에 그대로!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>아,, 올해는 진짜 전세 이사 가야하는데 ㅠㅠ 젱장,, 집값 내년엔 더 비싸게찌????</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음뉴스 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @n13KkW9bpAuhy6G: @YoungpyoHong 제헌절의 취지를 무색...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.420000e+18</td>\n",
       "      <td>'중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document  label  \\\n",
       "150  1.420000e+18  “안타깝지만 집값 안 떨어집니다, 더 오를 겁니다” : 뉴스 : 동아닷컴 https...      1   \n",
       "151  1.420000e+18  버블세븐 때처럼 '집값 거품론' 펴는 정부, 그런데도 시장은 정반대로 가고 있다 [...      1   \n",
       "152  1.420000e+18  https://t.co/X2BPCzsDxy\\n공급폭탄이 정답입니다.\\n오르는 집값 ...      0   \n",
       "153  1.420000e+18  RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...      0   \n",
       "154  1.420000e+18  정부에서 아파트 가격 상한선을 결정하는 중국, 폭등 집값을 40%↓ https://...      0   \n",
       "155  1.420000e+18  RT @S710_VID: 마당 있는 큰 집 나올 때마다 부러워하며 가격부터 물어보는...      0   \n",
       "156  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "157  1.420000e+18  RT @hbeekay: @NYjjNY 맞는 말씀이시네요. ‘위기가 기회’라는 말을 ...      1   \n",
       "158  1.420000e+18                              2Q 집값 서울은 부진전세 상승세 지속      1   \n",
       "159  1.420000e+18  ＂해외 유학 대신＂ 강남 부자들 우르르…13억 뚫은 제주 집값 (출처 : 한국경제 ...      1   \n",
       "160  1.420000e+18  RT @nonunsea: 재산서 고지서 날아왔는데 저년 대비 20% 경감된거 같다....      0   \n",
       "161  1.420000e+18  RT @janghyeyeong: 대한민국 국회 기재위에서 부자감세 종부세 개악을 반...      0   \n",
       "162  1.420000e+18                                 @oo_moo1 집값 나눠내자^^      0   \n",
       "163  1.420000e+18  대전하수구 여수오픈행사 사상욕실인테리어 부안군혈당계 무주비만클리닉 홍천유리복원 대명...      0   \n",
       "164  1.420000e+18                        RT @imnabyim: 만양 집값 오르겠네 이거      1   \n",
       "165  1.420000e+18                                      만양 집값 오르겠네 이거      1   \n",
       "166  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "167  1.420000e+18  RT @nonunsea: 재산서 고지서 날아왔는데 저년 대비 20% 경감된거 같다....      0   \n",
       "168  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "169  1.420000e+18  이정부처럼 집값잡겠다고???\\n\\n땅이 나라것인 공산당 나라에 집값이 평당 2억이란...      1   \n",
       "170  1.420000e+18  &lt;옆집에 수상한 과부가 살고있습니다?!&gt;\\n\\n자수성가하여 한적한 라이프...      0   \n",
       "171  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "172  1.420000e+18  '집값 전쟁' 놓고 국회서 설전…참여정부 인사는 쓴소리 / 연합뉴스 (Yonhapn...      0   \n",
       "173  1.420000e+18  아니 근데 내가 이걸 웃을수가 없는게.. 실제로 베이징 집값이 너무 어마어마해서 주...      1   \n",
       "174  1.420000e+18  20세기나 노무현때는 본질적으로 수도권의 압도적인 팽창이 집값 상승의 가장 큰 부분...      0   \n",
       "175  1.420000e+18  예나 지금이나 대충 그래도 돈 좀 모았음의 기준이 1억인데 그거가지고 집샀으면 몇억...      1   \n",
       "176  1.420000e+18  @NYjjNY 맞는 말씀이시네요. ‘위기가 기회’라는 말을 이따위로 쓰다니… 집투로...      1   \n",
       "177  1.420000e+18  음 일단 시즈는 이혼녀임. 왜냐 그게더 맛있으니까\\n\\n이혼후 위자료랑 이것저것 대...      0   \n",
       "178  1.420000e+18  집값이 더 오르는 말든 지금 상승기 한번 거하게 지나고 단기고점인데 지금 들어가도 ...      1   \n",
       "179  1.420000e+18  조기숙 “文, 집값 폭락 사지 말라 언급”…내용은? | 뉴스A 라이브 - YouTu...      0   \n",
       "180  1.420000e+18  조기숙 \"문 대통령, 日처럼 집값 폭락할테니 집 사지 말라 해\" / YTN - Yo...      0   \n",
       "181  1.420000e+18  선택지는 두 가지임\\n\\n1. 테이퍼링, 금리인상 등으로 집값을 내려오게 유도하던가...      0   \n",
       "182  1.420000e+18                정말 딱 집값만 다 갚으면 나는 공방차려야지 공방하고 평생 살래      0   \n",
       "183  1.420000e+18  \"중국판 대치동'에 폭등..\\n\\n최근 베이징 등 주요 도시에서 좋은 학교에 진학할...      1   \n",
       "184  1.420000e+18     이렇게 매미도 많고 모기도 많고 에어컨 방에 한개씩 없는 집인데 집값은 더럽게 높네      1   \n",
       "185  1.420000e+18  윤은 그냥 평범한 회사원이엇는데 최근에 회사가 테러 집단 때문에 망해서 지금은 일단...      0   \n",
       "186  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "187  1.420000e+18   요즘 지옥 집값이 너무 살신적이라~\\n1원 더 늘리는 부동산 정책을 필까 하는데 말야~      1   \n",
       "188  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "189  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "190  1.420000e+18  심즈 프리플레이 가구수 늘어날때마다 집값오르는거 개빡침..현실의 부동산이 게임에 그대로!      1   \n",
       "191  1.420000e+18   아,, 올해는 진짜 전세 이사 가야하는데 ㅠㅠ 젱장,, 집값 내년엔 더 비싸게찌????      1   \n",
       "192  1.420000e+18  '중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음뉴스 https://t...      1   \n",
       "193  1.420000e+18  RT @n13KkW9bpAuhy6G: @YoungpyoHong 제헌절의 취지를 무색...      0   \n",
       "194  1.420000e+18  RT @goRaikkonen: \"해외 유학 대신\" 강남 부자들 우르르…13억 뚫은 ...      1   \n",
       "195  1.420000e+18  RT @lwhsongri5: 英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 ...      1   \n",
       "196  1.420000e+18  RT @djWJrnwjWJrn: 집값은 아무래도 떨어지지않을건가봄 https://t...      1   \n",
       "197  1.420000e+18  英 집값지수 13% 급등… ‘中억압 탈출’ 홍콩인들이 런던 집 2000채 사들여 h...      1   \n",
       "198  1.420000e+18  '중국판 대치동'에 폭등..시진핑까지 나선 집값 잡기 | 다음 뉴스 https://...      1   \n",
       "\n",
       "     predicted  \n",
       "150          1  \n",
       "151          0  \n",
       "152          1  \n",
       "153          0  \n",
       "154          1  \n",
       "155          1  \n",
       "156          1  \n",
       "157          0  \n",
       "158          0  \n",
       "159          0  \n",
       "160          1  \n",
       "161          1  \n",
       "162          1  \n",
       "163          0  \n",
       "164          0  \n",
       "165          1  \n",
       "166          1  \n",
       "167          0  \n",
       "168          0  \n",
       "169          1  \n",
       "170          0  \n",
       "171          1  \n",
       "172          0  \n",
       "173          1  \n",
       "174          1  \n",
       "175          0  \n",
       "176          0  \n",
       "177          1  \n",
       "178          1  \n",
       "179          1  \n",
       "180          1  \n",
       "181          0  \n",
       "182          1  \n",
       "183          1  \n",
       "184          0  \n",
       "185          1  \n",
       "186          0  \n",
       "187          0  \n",
       "188          0  \n",
       "189          0  \n",
       "190          1  \n",
       "191          1  \n",
       "192          1  \n",
       "193          1  \n",
       "194          1  \n",
       "195          1  \n",
       "196          1  \n",
       "197          1  \n",
       "198          1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['predicted']= lab\n",
    "dataset_test.head(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 19\n"
     ]
    }
   ],
   "source": [
    "pos= 0\n",
    "neg=0\n",
    "\n",
    "for item in dataset_test['predicted']:\n",
    "    if item==1:\n",
    "        pos+=1\n",
    "    else:\n",
    "        neg+=1\n",
    "        \n",
    "        \n",
    "print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5789473684210527"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight= pos/neg\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
